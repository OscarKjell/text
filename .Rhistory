# Process each batch and store results i=1
batch_results <- list()
for (i in seq_along(batches)) {
batch_message <- sprintf("Processing batch %d/%d\n", i, length(batches))
message(colourise(batch_message, "blue"))
batch_texts <- batches[[i]]
#batch_texts <- batch[["satisfactionwords"]]
# Process batch with error handling
if(dlatk_method == FALSE){
batch_result <- tryCatch(
text_embed(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
tokens_select = tokens_select,
tokens_deselect = tokens_deselect,
decontextualize = decontextualize,
model_max_length = model_max_length,
max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
logging_level = logging_level
, ...
), # ADD TODO , ... for testing:
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
if(dlatk_method == TRUE){
# Process batch with error handling
batch_result <- tryCatch(
text_embed_dlatk(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
#    aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
#    aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
#   keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
#    tokens_select = tokens_select,
#    tokens_deselect = tokens_deselect,
#    decontextualize = decontextualize,
model_max_length = model_max_length,
#    max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
trust_remote_code = trust_remote_code,
logging_level = logging_level,
batch_size =as.integer(batch_size)
, ...
),
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
batch_results[[i]] <- batch_result
T2 <- Sys.time()
time_from_starts <- round(as.numeric(difftime(T2, T1, units = "mins")), 3)
time_from_message <- paste("Minutes from start: ", time_from_starts)
message(colourise(time_from_message, "green"))
batches_left <- length(batches) - i
mean_time_per_batch <- time_from_starts/i
estimated_time_left <- mean_time_per_batch * batches_left
estimation_message <- paste0("Estimated embedding time left = ", estimated_time_left, " minutes")
message(colourise(estimation_message, "black"))
}
final_result <- combine_textEmbed_results(
batch_results,
aggregation = aggregation_from_tokens_to_word_types)
return(final_result)
}
#' Change dimension names
#'
#' textDimName() changes the names of the dimensions in the word embeddings.
#' @param word_embeddings List of word embeddings
#' @param dim_names (boolean) If TRUE the word embedding name will be attached to the name of each dimension;
#' is FALSE, the attached part of the name will be removed.
#' @return Word embeddings with changed names.
#' @examples
#' \donttest{
#' # Note that dimensions are called Dim1_harmonytexts etc.
#' word_embeddings_4$texts$harmonytexts
#' # Here they are changed to just Dim
#' w_e_T <- textDimName(word_embeddings_4$texts["harmonytexts"],
#'   dim_names = FALSE
#' )
#' # Here they are changed back
#' w_e_F <- textDimName(w_e_T, dim_names = TRUE)
#' }
#' @seealso see \code{\link{textEmbed}}
#' @export
textDimName <- function(word_embeddings,
dim_names = TRUE) {
tokens <- NULL
word_type <- NULL
x_is_tibble <- tibble::is_tibble(word_embeddings)
if (x_is_tibble) word_embeddings <- list(word_embeddings)
# Remove singlewords_we if it exist
if (!is.null(word_embeddings$word_type)) {
word_type <- word_embeddings$word_type
word_embeddings$word_type <- NULL
}
if (!is.null(word_embeddings$tokens)) {
tokens <- word_embeddings$tokens
word_embeddings$tokens <- NULL
}
# i_row = 1 dim_name=TRUE
if (dim_names) {
for (i_row in seq_len(length(word_embeddings))) {
colnames(word_embeddings[[i_row]]) <- paste0(
names(word_embeddings[[i_row]]),
"_",
names(word_embeddings)[[i_row]]
)
}
}
if (!dim_names) {
for (i_row in seq_len(length(word_embeddings))) {
target_variables_names <- colnames(word_embeddings[[i_row]])
# Select everything BEFORE the first _ (i.e., the Dim1, etc.)
variable_names <- sub("\\_.*", "", target_variables_names)
colnames(word_embeddings[[i_row]]) <- variable_names
}
}
# Attach word embeddings again
if (!is.null(word_type)) {
word_embeddings$word_type <- word_type
}
# Attach word embeddings again
if (!is.null(tokens)) {
word_embeddings$tokens <- tokens
}
# Return tibble if x is a tibble (and not a list)
if (x_is_tibble) word_embeddings <- word_embeddings[[1]]
return(word_embeddings)
}
T1 <- Sys.time()
if(!tibble::is_tibble(texts)){
texts <- tibble::tibble(texts = texts)
}
# Split texts into batches
split_into_batches <- function(data, batch_size) {
split(data, ceiling(seq_along(1:nrow(data)) / batch_size))
}
batches <- split_into_batches(texts, batch_size)
texts = "hello"
model = "mixedbread-ai/mxbai-embed-large-v1"
layers = -2
dim_name = TRUE
aggregation_from_layers_to_tokens = "concatenate"
aggregation_from_tokens_to_texts = "mean"
aggregation_from_tokens_to_word_types = NULL
keep_token_embeddings = TRUE
batch_size = 100
remove_non_ascii = TRUE
tokens_select = NULL
tokens_deselect = NULL
decontextualize = FALSE
model_max_length = NULL
max_token_to_sentence = 4
tokenizer_parallelism = FALSE
device = "cpu"
hg_gated = FALSE
hg_token = Sys.getenv("HUGGINGFACE_TOKEN",
unset = "")
logging_level = "error"
dlatk_method = FALSE
trust_remote_code = FALSE
T1 <- Sys.time()
if(!tibble::is_tibble(texts)){
texts <- tibble::tibble(texts = texts)
}
# Split texts into batches
split_into_batches <- function(data, batch_size) {
split(data, ceiling(seq_along(1:nrow(data)) / batch_size))
}
batches <- split_into_batches(texts, batch_size)
# Process each batch and store results i=1
batch_results <- list()
for (i in seq_along(batches)) {
batch_message <- sprintf("Processing batch %d/%d\n", i, length(batches))
message(colourise(batch_message, "blue"))
batch_texts <- batches[[i]]
#batch_texts <- batch[["satisfactionwords"]]
# Process batch with error handling
if(dlatk_method == FALSE){
batch_result <- tryCatch(
text_embed(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
tokens_select = tokens_select,
tokens_deselect = tokens_deselect,
decontextualize = decontextualize,
model_max_length = model_max_length,
max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
logging_level = logging_level
, ...
), # ADD TODO , ... for testing:
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
if(dlatk_method == TRUE){
# Process batch with error handling
batch_result <- tryCatch(
text_embed_dlatk(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
#    aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
#    aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
#   keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
#    tokens_select = tokens_select,
#    tokens_deselect = tokens_deselect,
#    decontextualize = decontextualize,
model_max_length = model_max_length,
#    max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
trust_remote_code = trust_remote_code,
logging_level = logging_level,
batch_size =as.integer(batch_size)
, ...
),
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
batch_results[[i]] <- batch_result
T2 <- Sys.time()
time_from_starts <- round(as.numeric(difftime(T2, T1, units = "mins")), 3)
time_from_message <- paste("Minutes from start: ", time_from_starts)
message(colourise(time_from_message, "green"))
batches_left <- length(batches) - i
mean_time_per_batch <- time_from_starts/i
estimated_time_left <- mean_time_per_batch * batches_left
estimation_message <- paste0("Estimated embedding time left = ", estimated_time_left, " minutes")
message(colourise(estimation_message, "black"))
}
# Process batch with error handling
if(dlatk_method == FALSE){
batch_result <- tryCatch(
text_embed(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
tokens_select = tokens_select,
tokens_deselect = tokens_deselect,
decontextualize = decontextualize,
model_max_length = model_max_length,
max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
logging_level = logging_level
#     , ...
), # ADD TODO , ... for testing:
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
if(dlatk_method == TRUE){
# Process batch with error handling
batch_result <- tryCatch(
text_embed_dlatk(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
#    aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
#    aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
#   keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
#    tokens_select = tokens_select,
#    tokens_deselect = tokens_deselect,
#    decontextualize = decontextualize,
model_max_length = model_max_length,
#    max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
trust_remote_code = trust_remote_code,
logging_level = logging_level,
batch_size =as.integer(batch_size)
, ...
),
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
batch_results[[i]] <- batch_result
T2 <- Sys.time()
time_from_starts <- round(as.numeric(difftime(T2, T1, units = "mins")), 3)
time_from_message <- paste("Minutes from start: ", time_from_starts)
message(colourise(time_from_message, "green"))
batches_left <- length(batches) - i
mean_time_per_batch <- time_from_starts/i
estimated_time_left <- mean_time_per_batch * batches_left
estimation_message <- paste0("Estimated embedding time left = ", estimated_time_left, " minutes")
message(colourise(estimation_message, "black"))
final_result <- combine_textEmbed_results(
batch_results,
aggregation = aggregation_from_tokens_to_word_types)
comment(final_result$texts)
comment(final_result$texts$texts)
#'
#' # Retrieve the saved word embeddings.
#' word_embeddings <- readRDS("word_embeddings.rds")
#' }
#'
#' @seealso See \code{\link{textEmbedLayerAggregation}}, \code{\link{textEmbedRawLayers}} and
#' \code{\link{textDimName}}.
#' @importFrom reticulate source_python
#' @importFrom utils modifyList
#' @export
textEmbed <- function(
texts,
model = "bert-base-uncased",
layers = -2,
dim_name = TRUE,
aggregation_from_layers_to_tokens = "concatenate",
aggregation_from_tokens_to_texts = "mean",
aggregation_from_tokens_to_word_types = NULL,
keep_token_embeddings = TRUE,
batch_size = 100,
remove_non_ascii = TRUE,
tokens_select = NULL,
tokens_deselect = NULL,
decontextualize = FALSE,
model_max_length = NULL,
max_token_to_sentence = 4,
tokenizer_parallelism = FALSE,
device = "cpu",
hg_gated = FALSE,
hg_token = Sys.getenv("HUGGINGFACE_TOKEN",
unset = ""),
logging_level = "error",
dlatk_method = FALSE,
trust_remote_code = FALSE,
...) {
T1 <- Sys.time()
if(!tibble::is_tibble(texts)){
texts <- tibble::tibble(texts = texts)
}
# Split texts into batches
split_into_batches <- function(data, batch_size) {
split(data, ceiling(seq_along(1:nrow(data)) / batch_size))
}
batches <- split_into_batches(texts, batch_size)
# Process each batch and store results i=1
batch_results <- list()
for (i in seq_along(batches)) {
batch_message <- sprintf("Processing batch %d/%d\n", i, length(batches))
message(colourise(batch_message, "blue"))
batch_texts <- batches[[i]]
#batch_texts <- batch[["satisfactionwords"]]
# Process batch with error handling
if(dlatk_method == FALSE){
batch_result <- tryCatch(
text_embed(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
tokens_select = tokens_select,
tokens_deselect = tokens_deselect,
decontextualize = decontextualize,
model_max_length = model_max_length,
max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
logging_level = logging_level
, ...
), # ADD TODO , ... for testing:
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
if(dlatk_method == TRUE){
# Process batch with error handling
batch_result <- tryCatch(
text_embed_dlatk(
texts = batch_texts,
model = model,
layers = layers,
dim_name = dim_name,
#    aggregation_from_layers_to_tokens = aggregation_from_layers_to_tokens,
aggregation_from_tokens_to_texts = aggregation_from_tokens_to_texts,
#    aggregation_from_tokens_to_word_types = aggregation_from_tokens_to_word_types,
#   keep_token_embeddings = keep_token_embeddings,
remove_non_ascii = remove_non_ascii,
#    tokens_select = tokens_select,
#    tokens_deselect = tokens_deselect,
#    decontextualize = decontextualize,
model_max_length = model_max_length,
#    max_token_to_sentence = max_token_to_sentence,
tokenizer_parallelism = tokenizer_parallelism,
device = device,
hg_gated = hg_gated,
hg_token = hg_token,
trust_remote_code = trust_remote_code,
logging_level = logging_level,
batch_size =as.integer(batch_size)
, ...
),
error = function(e) {
message(sprintf("Error in batch %d: %s", i, e$message))
return(NULL)
}
)
}
batch_results[[i]] <- batch_result
T2 <- Sys.time()
time_from_starts <- round(as.numeric(difftime(T2, T1, units = "mins")), 3)
time_from_message <- paste("Minutes from start: ", time_from_starts)
message(colourise(time_from_message, "green"))
batches_left <- length(batches) - i
mean_time_per_batch <- time_from_starts/i
estimated_time_left <- mean_time_per_batch * batches_left
estimation_message <- paste0("Estimated embedding time left = ", estimated_time_left, " minutes")
message(colourise(estimation_message, "black"))
}
final_result <- combine_textEmbed_results(
batch_results,
aggregation = aggregation_from_tokens_to_word_types)
return(final_result)
}
emb1 <- textEmbed(
texts = "hello",
model = "mixedbread-ai/mxbai-embed-large-v1"
)
comment(emb1$texts$texts)
emb1 <- text::textEmbed(
texts = "hello",
model = "mixedbread-ai/mxbai-embed-large-v1"
)
comment(emb1$texts$texts)
emb1 <- text::textEmbed(
texts = "hello",
model = "mixedbread-ai/mxbai-embed-large-v1"
)
comment(emb1$texts$texts)
remove.packages("text")
.rs.restartR()
library(text)
emb1 <- text::textEmbed(
texts = "hello",
model = "mixedbread-ai/mxbai-embed-large-v1"
)
comment(emb1$texts$texts)
remove.packages("text")
library(text)
.rs.restartR()
library(text)
